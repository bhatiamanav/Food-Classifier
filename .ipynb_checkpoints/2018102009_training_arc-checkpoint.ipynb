{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeJxwu6Q5vmY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asIF1npB6GMG",
    "outputId": "2a035bc9-3799-4e71-e36a-4572daf6d6af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING THE TRAINING DATASET BY COMBINING THE LABELS AND IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5oXdvek51y6"
   },
   "outputs": [],
   "source": [
    "class Image_Train_Data(Dataset):\n",
    "    def __init__(self,data_list,data_dir,transform=None):\n",
    "        super().__init__()\n",
    "        self.data_list = data_list\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (self.data_list.shape[0])\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        img_name,label = self.data_list.iloc[item]\n",
    "        img_path = os.path.join(self.data_dir,img_name)\n",
    "        img = cv2.imread(img_path,1)\n",
    "        img = cv2.resize(img,(224,224))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return \n",
    "        {\n",
    "              'image' : img,\n",
    "              'label' : torch.tensor(label)\n",
    "\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWKHv74L6CJo"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "batch = 128\n",
    "train_path = '/content/drive/My Drive/train_images/train_images'\n",
    "test_path = '/content/drive/My Drive/test_images/test_images'\n",
    "train_labels = pd.read_csv('/content/drive/My Drive/train.csv')\n",
    "test_ids = pd.read_csv('/content/drive/My Drive/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYCRx77B69bx",
    "outputId": "234577f4-33dd-4c32-c066-60f5819d3161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55 41 12 ... 14 12 21]\n"
     ]
    }
   ],
   "source": [
    "enc = LabelEncoder()\n",
    "training_labels = enc.fit_transform(train_labels['ClassName'])\n",
    "print(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,0.5,0.5) , (0.5,0.5,0.5))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb3XBqdw7ls6"
   },
   "outputs": [],
   "source": [
    "train_l = train_labels\n",
    "train_l['ClassName'] = training_labels\n",
    "\n",
    "train_data = Image_Train_Data(data_list = train_l, data_dir = train_path, transform = transforms_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING A RANDOM SUBSET SAMPLER TO GET TRAINING AND VALIDATION SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_len = len(train_data)\n",
    "idxs = list(range(len(train_data_len))\n",
    "\n",
    "def tr_val_sampler(val_sz, idxs):\n",
    "    np.random.shuffle(idxs)\n",
    "    val_idx = (val_size*train_data_len)\n",
    "    val_idx = int(val_idx)\n",
    "            \n",
    "    train_idxs = idxs[val_idx:len(train_data_len) - 1]\n",
    "    val_idxs = idxs[0:val_idx]\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idxs)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idxs)\n",
    "            \n",
    "    return train_sampler, valid_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(train_data, train_sampler, valid_sampler, batch_size):\n",
    "    train_loader = DataLoader(train_data, batch_size = batch_size, sampler = train_sampler)\n",
    "    valid_loader = DataLoader(train_data, batch_size = batch_size, sampler = valid_sampler)\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkOBOkPw7_-W"
   },
   "outputs": [],
   "source": [
    "val_sz = 0.2\n",
    "\n",
    "train_sampler, valid_sampler = tr_val_sampler(val_sz, idxs)\n",
    "\n",
    "train_loader, valid_loader = data_loader(train_data, train_sampler, valid_sampler, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifBAwgmC8J6u",
    "outputId": "381a6aca-a0dc-4ac4-8d44-8e5a87d1aef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING THE MODELS\n",
    "\n",
    "We define the models as-\n",
    "   <b> <ol>\n",
    "        <li> Basic Model, with 3 Convolutional Layers, 2 Max Pooling layers, and 3 Linear Layers with RELU activation functions.</li>\n",
    "         <li> Average Pooling Model, with 3 Convolutional Layers, 2 Average Pooling layers, and 3 Linear Layers with RELU activation functions.</li>\n",
    "         <li> Batch Norm Model, with 2 Convolutional Layers, 2 Max Pooling layers, 1 2D Batch Norm Layer of size 16 and 3 Linear Layers with RELU activation functions.</li>\n",
    "         <li> Dropout Model, with 2 Convolutional Layers, 2 Max Pooling layers, and 1 Dropout layer of value 0.25 added after the first linear layer.3 Linear Layers with RELU activation functions.</li>\n",
    "         <li> Sigmoid Model, with 3 Convolutional Layers, 2 Max Pooling layers, and 3 Linear Layers with Sigmoid activation functions.</li>\n",
    "         <li> Leaky RELU Model, with 3 Convolutional Layers, 2 Max Pooling layers, and 3 Linear Layers with Leaky RELU activation functions.</li>\n",
    "         <li>Additional Layers Model, with 3 Convolutional Layers, 3 Max Pooling layers, and 4 Linear Layers with RELU activation functions.</li>\n",
    "    </ol></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CLoPbEg8Mm6",
    "outputId": "6b1bbe62-f70e-45e9-be4e-f473c5fc99be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(59536, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=59536, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=61, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Basic_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Basic_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(16*61*61,120,1)\n",
    "        self.fc1 = nn.Linear(16 * 61 * 61, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 61)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 16*61*61)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Avg_Pool_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Avg_Pool_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool1 = nn.AvgPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool2 = nn.AvgPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(16*61*61,120,1)\n",
    "        self.fc1 = nn.Linear(16 * 61 * 61, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 61)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*61*61)\n",
    "        x = self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))\n",
    "        return x\n",
    "    \n",
    "class Batch_Norm_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Batch_Norm_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.batch= nn.BatchNorm2d(16)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 61 * 61, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 61)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*61*61)\n",
    "        x = self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))\n",
    "        return x\n",
    "    \n",
    "class DropOut_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DropOut_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 61 * 61, 120)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 61)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*61*61)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc3(F.relu(self.fc2(x)))\n",
    "        return x\n",
    "    \n",
    "class Sigmoid_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 61 * 61, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 61)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.sigmoid(self.conv1(x)))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 16*61*61)\n",
    "        x = self.fc3(torch.sigmoid(self.fc2(torch.sigmoid(self.fc1(x)))))\n",
    "        return x\n",
    "    \n",
    "class Leaky_RELU_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Leaky_RELU_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 61 * 61, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 61)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = nn.LeakyReLU(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.pool2(nn.LeakyReLU(self.conv2(x)))\n",
    "        x = x.view(-1, 16*61*61)\n",
    "        x = self.fc3(nn.LeakyReLU(self.fc2(nn.LeakyReLU(self.fc1(x)))))\n",
    "        return x\n",
    "    \n",
    "class Addntl_Layer_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Addntl_Layer_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(16, 8, 5)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(8 * 28 * 28, 120)\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, 84)\n",
    "        self.fc4 = nn.Linear(84, 61)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 8* 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing the Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Basic_Model().to(device)\n",
    "error = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pziWgc0B8Sv9"
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "def train_step(model,train_loader, optimizer, train_loss):\n",
    "    for images in tqdm(train_loader):\n",
    "        data = images['img'].squeeze(0).to(device)\n",
    "        target = images['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = error(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_size = data.size(0)\n",
    "        train_loss += loss.item()*loss_size\n",
    "        \n",
    "    return model,train_loss\n",
    "\n",
    "def valid_step(model, valid_loader, optimizer, valid_loss):\n",
    "    for images in tqdm(valid_loader):\n",
    "        data = images['img'].squeeze(0).to(device)\n",
    "        target = images['label'].to(device)\n",
    "        output = model(data)\n",
    "        loss = error(output, target)\n",
    "        loss_size = data.size(0)\n",
    "        valid_loss += loss.item()*loss_size\n",
    "        \n",
    "    return model, valid_loss\n",
    "\n",
    "def test_step(model, valid_loader, pred_list, true_list):\n",
    "    for images in valid_loader:\n",
    "        data = images['img'].squeeze(0).to(device)\n",
    "        target = images['label'].to(device)\n",
    "        outputs = model(data)\n",
    "        \n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        pred_label = predicted.detach().cpu().numpy()\n",
    "        for i in pred_label:\n",
    "            pred_list.append(i)\n",
    "            \n",
    "        target_label = target.detach().cpu().numpy()\n",
    "        for i in target_label:\n",
    "            true_list.append(i)\n",
    "            \n",
    "    return pred_list, true_list\n",
    "    \n",
    "    \n",
    "def train(model,optimizer,min_valid_loss,train_losses,valid_losses):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        print (\"Training starts here for epoch \" + str(epoch))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        model, train_loss = train_step(model, train_loader, optimizer, train_loss)   \n",
    "            \n",
    "        model.eval()\n",
    "        print (\"Validation starts here for epoch \" + str(epoch))\n",
    "        \n",
    "        model, valid_loss = valid_step(model, valid_loader, optimizer, valid_loss)\n",
    "\n",
    "        train_losses.append(train_loss/len(train_loader.sampler))\n",
    "        valid_losses.append(valid_loss/len(valid_loader.sampler))\n",
    "\n",
    "        print('\\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(train_loss, valid_loss))\n",
    "\n",
    "        if valid_loss <= min_valid_loss:\n",
    "            min_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best_model_so_far.pth')\n",
    "            \n",
    "def test(model):\n",
    "    model.load_state_dict(torch.load('best_model_so_far.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_list, true_list = test_step(model, valid_loader, [], [])\n",
    "    return true_list,pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PujrBTr48ZxT",
    "outputId": "34bbdbb9-7b76-4e60-f90a-5e00b3a48ff3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [28:06<00:00, 28.59s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [06:33<00:00, 26.26s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \tTraining Loss: 3.781249 \tValidation Loss: 3.519950\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:15<00:00,  1.28s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.18s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tTraining Loss: 3.381973 \tValidation Loss: 3.299441\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.27s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 \tTraining Loss: 3.066848 \tValidation Loss: 3.188422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model,optimizer,np.Inf,[],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6BZVQQR8e8c"
   },
   "outputs": [],
   "source": [
    "correct_list, pred_list = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4tRWJVtFNJc",
    "outputId": "85d26b9e-3f51-4c2a-94c1-2be18f28bd4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.22424892703862662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score \n",
    "print(\"F1 score :\",f1_score(correct_list,pred_list,average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFFECT OF OPTIMISERS\n",
    "\n",
    "We checked the model for ADAM optimiser, SGD optimiser and AdaGrad optimiser and checked the results, and got the highest results in AdaGrad optimisers, followed by ADAM and the worst case scenario in SGD.\n",
    "\n",
    "<b>\n",
    "AdaGrad results = 0.22639484978540772<br>\n",
    "ADAM results =0.22424892703862662<br>\n",
    "SGD results = 0.18830472103004292<br>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwUG0-lvFTO-",
    "outputId": "6b5ce866-0b5c-407b-b459-716976b75543"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.27s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.17s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \tTraining Loss: 3.540056 \tValidation Loss: 3.486708\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.27s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.18s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tTraining Loss: 3.192706 \tValidation Loss: 3.289073\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:13<00:00,  1.25s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 \tTraining Loss: 2.940591 \tValidation Loss: 3.276210\n",
      "F1 score : 0.18830472103004292\n"
     ]
    }
   ],
   "source": [
    "optim2 = optim.SGD(model.parameters(),lr=0.001, momentum=0.9)\n",
    "\n",
    "train(model,optim2,np.Inf,[],[])\n",
    "\n",
    "correct_list_sgd, pred_list_sgd = test(model)\n",
    "print(\"F1 score :\",f1_score(correct_list_sgd,pred_list_sgd,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyoxhnbDFbol",
    "outputId": "a89fc3ca-3370-4d99-b805-0fd3e998752f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.26s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.18s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \tTraining Loss: 3.438911 \tValidation Loss: 3.256973\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.27s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.18s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tTraining Loss: 2.501815 \tValidation Loss: 3.417650\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.26s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 \tTraining Loss: 1.912146 \tValidation Loss: 3.380934\n",
      "F1 score : 0.22639484978540772\n"
     ]
    }
   ],
   "source": [
    "optim3 = optim.Adagrad(model.parameters(),lr=0.01)\n",
    "train(model,optim3,np.Inf,[],[])\n",
    "\n",
    "correct_list_ada, pred_list_ada = test(model)\n",
    "print(\"F1 score :\",f1_score(correct_list_ada,pred_list_ada,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLrSyeO6Gn50",
    "outputId": "a33df306-4722-45a3-8d10-7e0f1e31fc28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net1(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv3): Conv2d(59536, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=59536, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=61, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model1 = Avg_Pool_Model().to(device)\n",
    "error = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model1.parameters())\n",
    "print (model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xXsZrd5HcQb",
    "outputId": "2716159b-c023-459a-f813-25b996a3c05f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.26s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.17s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \tTraining Loss: 3.644646 \tValidation Loss: 3.398835\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:15<00:00,  1.27s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.17s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tTraining Loss: 3.179275 \tValidation Loss: 3.228299\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.27s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 \tTraining Loss: 2.901254 \tValidation Loss: 3.064934\n",
      "F1 score : 0.23336909871244635\n"
     ]
    }
   ],
   "source": [
    "train(model1,optimizer,device,np.Inf,[],[])\n",
    "\n",
    "correct_list_avg, pred_list_avg = test(model1)\n",
    "print(\"F1 score :\",f1_score(correct_list_avg,pred_list_avg,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TseoOj14HfUN",
    "outputId": "34e54103-d2f9-4dc8-faa1-a414f985e931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net2(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (batch): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=59536, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=61, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model2 = Batch_Norm_Model().to(device)\n",
    "\n",
    "error = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model2.parameters())\n",
    "\n",
    "print (model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UsFE0JvZJ40n",
    "outputId": "11b1f90a-7dc5-4a26-d32f-cdbd03bbec47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.27s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.17s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \tTraining Loss: 4.373112 \tValidation Loss: 3.691734\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.27s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.19s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tTraining Loss: 3.575482 \tValidation Loss: 3.500795\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:15<00:00,  1.28s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 \tTraining Loss: 3.344847 \tValidation Loss: 3.368951\n",
      "F1 score : 0.20064377682403434\n"
     ]
    }
   ],
   "source": [
    "train(model2,optimizer,np.Inf,[],[])\n",
    "\n",
    "correct_list_batch, pred_list_batch = test(model2)\n",
    "print(\"F1 score :\",f1_score(correct_list_batch,pred_list_batch,average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFFECT OF AVERAGE POOLING AND BATCH NORM RESULTS\n",
    "\n",
    "<b>Average Pooling helps in the process as it improves the F1 score, proving it is a better pooling for this model than Max pooling.\n",
    "\n",
    "Batch Norm models are not particularly helpful as they reduce the F1 score by 0.02.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwVjONEgJ9Fr",
    "outputId": "659d089a-aac8-44b8-cff6-62574b770c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net3(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=59536, out_features=120, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=61, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model3 = DropOut_Model().to(device)\n",
    "\n",
    "error = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model3.parameters())\n",
    "\n",
    "print (model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsJ7ZiqtLJ5u",
    "outputId": "1bea5ac4-06bc-4309-d919-9ca2494214db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.26s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.17s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \tTraining Loss: 3.712224 \tValidation Loss: 3.434507\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:15<00:00,  1.27s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.18s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tTraining Loss: 3.322580 \tValidation Loss: 3.201369\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.26s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 \tTraining Loss: 3.116546 \tValidation Loss: 3.052390\n",
      "F1 score : 0.23819742489270387\n"
     ]
    }
   ],
   "source": [
    "train(model3,optimizer,np.Inf,[],[])\n",
    "\n",
    "correct_list_drop, pred_list_drop = test(model3)\n",
    "print(\"F1 score :\",f1_score(correct_list_drop,pred_list_drop,average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFFECT OF DROPOUT\n",
    "\n",
    "<b>Dropout causes quite a nice effect, beating all previous results as it eases computation and makes the network more robust, hence helping in increasing the efficiency of the model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4cXKNt4LP7D",
    "outputId": "a58d746a-ceaf-4917-eace-eafc7ee3356e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net4(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=59536, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=61, bias=True)\n",
      ")\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.26s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.17s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \tTraining Loss: 3.852116 \tValidation Loss: 3.768141\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:15<00:00,  1.28s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.18s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tTraining Loss: 3.792972 \tValidation Loss: 3.770380\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.26s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 \tTraining Loss: 3.792338 \tValidation Loss: 3.769854\n",
      "F1 score : 0.09656652360515021\n"
     ]
    }
   ],
   "source": [
    "model4 = Sigmoid_Model().to(device)\n",
    "error = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model4.parameters())\n",
    "print (model4)\n",
    "\n",
    "train(model4,optimizer,device,np.Inf,[],[])\n",
    "correct_list7, pred_list7 = test(model4)\n",
    "print(\"F1 score :\",f1_score(correct_list7,pred_list7,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRrksJqjNssr",
    "outputId": "e131c35c-366e-4328-b713-a1d4d06b92f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net6(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=59536, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=61, bias=True)\n",
      ")\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:15<00:00,  1.29s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.20s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \tTraining Loss: 3.620894 \tValidation Loss: 3.310150\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:15<00:00,  1.28s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.18s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tTraining Loss: 3.058695 \tValidation Loss: 3.114905\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:15<00:00,  1.29s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 \tTraining Loss: 2.553941 \tValidation Loss: 3.118278\n",
      "F1 score : 0.2113733905579399\n"
     ]
    }
   ],
   "source": [
    "model6 = Leaky_RELU_Model().to(device)\n",
    "error = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model6.parameters())\n",
    "print (model6)\n",
    "\n",
    "train(model6,optimizer,device,np.Inf,[],[])\n",
    "correct_list9, pred_list9 = test(model6)\n",
    "print(\"F1 score :\",f1_score(correct_list9,pred_list9,average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFFECT OF ACTIVATION FUNCTIONS\n",
    "\n",
    "<b>Sigmoid as an activation function is just not at all good as it forces the model into unfavourable conditions between 0 and 1. Leaky RELU functions slightly better than RELU(in general) as it solves the problem of vanishing gradients.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoWwVFxTO_KX",
    "outputId": "d285fb5a-9f56-4057-e4ec-3cd83e1d11d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net7(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(16, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=6272, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=84, bias=True)\n",
      "  (fc4): Linear(in_features=84, out_features=61, bias=True)\n",
      ")\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:15<00:00,  1.28s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.19s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \tTraining Loss: 3.770369 \tValidation Loss: 3.550940\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:15<00:00,  1.28s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.19s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tTraining Loss: 3.481498 \tValidation Loss: 3.362211\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:14<00:00,  1.27s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 \tTraining Loss: 3.341994 \tValidation Loss: 3.374594\n",
      "F1 score : 0.18991416309012876\n"
     ]
    }
   ],
   "source": [
    "model7 = Addntl_Layer_Model().to(device)\n",
    "error = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model7.parameters())\n",
    "print (model7)\n",
    "\n",
    "train(model7,optimizer,device,np.Inf,[],[])\n",
    "correct_list10, pred_list10 = test(model7)\n",
    "print(\"F1 score :\",f1_score(correct_list10,pred_list10,average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFFECT OF ADDITIONAL LAYERS\n",
    "\n",
    "<b>Additional Layers don't help here, just increasing the training time and causing less robustness and more overfitting to the model and overall deprecating the model efficiency.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train_2 = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,0.5,0.5) , (0.5,0.5,0.5)),\n",
    "            transforms.RandomRotation(10,fill=(0,)),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(),\n",
    "            transforms.RandomAffine(10,translate=(0.2,0.4))\n",
    "        ])\n",
    "\n",
    "train_data1 = Image_Train_Data(data_list= train_l,data_dir = train_path,transform = transforms_train_2)\n",
    "\n",
    "train_data_len1 = len(train_data1)\n",
    "idxs1 = list(range(len(train_data_len))\n",
    "            \n",
    "val_sz = 0.2\n",
    "\n",
    "train_sampler1, valid_sampler1 = tr_val_sampler(val_sz, idxs1)\n",
    "\n",
    "train_loader1, valid_loader1 = data_loader(train_data1, train_sampler1, valid_sampler1, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_step(model, optimizer, data, target):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = error(output,target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return optimizer, data\n",
    "\n",
    "def train_step_new(model,train_loader, optimizer, train_loss):\n",
    "    for images in tqdm(train_loader):\n",
    "        data = images['img'].squeeze(0).to(device)\n",
    "        target = images['label'].to(device)\n",
    "        \n",
    "        optimizer, data = optim_step(model, optimizer, data, target)\n",
    "        \n",
    "        loss_size = data.size(0)\n",
    "        train_loss += loss.item()*loss_size\n",
    "        \n",
    "    return model,train_loss\n",
    "\n",
    "def valid_step_new(model, valid_loader, optimizer, valid_loss):\n",
    "    for images in tqdm(valid_loader):\n",
    "        data = images['img'].squeeze(0).to(device)\n",
    "        target = images['label'].to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = error(output, target)\n",
    "        \n",
    "        loss_size = data.size(0)\n",
    "        valid_loss += loss.item()*loss_size\n",
    "        \n",
    "    return model, valid_loss\n",
    "\n",
    "def get_lists(predicted, target, pred_list, true_list):\n",
    "    pred_label = predicted.detach().cpu().numpy()\n",
    "    pred_label = pred_label.resize((pred_label.shape[0],1))\n",
    "    for i in range(len(pred_label)):\n",
    "        pred_list.append(pred_label[i][0])\n",
    "            \n",
    "    target_label = target.detach().cpu().numpy()\n",
    "    target_label = target_label.resize((target_label.shape[0],1))\n",
    "    for i in range(len(target_label)):\n",
    "        true_list.append(target_label[i][0])\n",
    "    \n",
    "    return pred_list, true_list\n",
    "\n",
    "def test_step_new(model, valid_loader, pred_list, true_list):\n",
    "    for images in valid_loader:\n",
    "        data = images['img'].squeeze(0).to(device)\n",
    "        target = images['label'].to(device)\n",
    "        outputs = model(data)\n",
    "        \n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        \n",
    "        pred_list, true_list = get_lists(predicted, target, pred_list, true_list)\n",
    "            \n",
    "    return pred_list, true_list\n",
    "    \n",
    "    \n",
    "def train_new(model,optimizer,min_valid_loss,train_losses,valid_losses):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        print (\"Training starts here for epoch \" + str(epoch))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        model, train_loss = train_step_new(model, train_loader, optimizer, train_loss)   \n",
    "            \n",
    "        model.eval()\n",
    "        print (\"Validation starts here for epoch \" + str(epoch))\n",
    "        \n",
    "        model, valid_loss = valid_step_new(model, valid_loader, optimizer, valid_loss)\n",
    "\n",
    "        train_losses.append(train_loss/len(train_loader.sampler))\n",
    "        valid_losses.append(valid_loss/len(valid_loader.sampler))\n",
    "\n",
    "        print('\\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(train_loss, valid_loss))\n",
    "\n",
    "        if valid_loss <= min_valid_loss:\n",
    "            min_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best_model_so_far.pth')\n",
    "            \n",
    "def test_new(model):\n",
    "    model.load_state_dict(torch.load('best_model_so_far.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_list, true_list = test_step_new(model, valid_loader, [], [])\n",
    "    return true_list,pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vu1gsRUQcYV",
    "outputId": "bbeecb7f-5757-49ac-e2ec-65ab6c8fc6cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [03:09<00:00,  3.21s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:45<00:00,  3.01s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \tTraining Loss: 3.589073 \tValidation Loss: 3.635884\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [03:10<00:00,  3.22s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:45<00:00,  3.02s/it]\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tTraining Loss: 3.583309 \tValidation Loss: 3.587222\n",
      "Training here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [03:09<00:00,  3.20s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation here...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:44<00:00,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 \tTraining Loss: 3.599558 \tValidation Loss: 3.632642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_new(model,optimizer,np.Inf,[],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAZ-FY_9R6uS",
    "outputId": "8d1474c1-9d67-4c00-b6cd-0e73ab837ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : 0.15450643776824036\n"
     ]
    }
   ],
   "source": [
    "correct_list_aug, pred_list_aug = test_new(model)\n",
    "print(\"F1 score :\",f1_score(correct_list_aug,pred_list_aug,average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqJW5VTJR8E3"
   },
   "source": [
    "<b>AUGMENTING DATA DOES NOT HELP THAT MUCH AS GENERAL MODELS WITH AUGMENTED DATA CAN'T FUNCTION MUCH BETTER UNLESS GIVEN TIME AND MEMORY TO TRAIN. THAT IS WHY, OUR BEST MODEL PROVIDES QUITE A SUBPAR EFFICIENCY ON THE AUGMENTED DATA MODEL.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
